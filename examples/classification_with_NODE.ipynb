{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utility Functions"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "def make_mixed_classification(n_samples, n_features, n_categories):\n",
    "    X,y = make_classification(n_samples=n_samples, n_features=n_features, random_state=42, n_informative=5)\n",
    "    cat_cols = random.choices(list(range(X.shape[-1])),k=n_categories)\n",
    "    num_cols = [i for i in range(X.shape[-1]) if i not in cat_cols]\n",
    "    for col in cat_cols:\n",
    "        X[:,col] = pd.qcut(X[:,col], q=4).codes.astype(int)\n",
    "    col_names = [] \n",
    "    num_col_names=[]\n",
    "    cat_col_names=[]\n",
    "    for i in range(X.shape[-1]):\n",
    "        if i in cat_cols:\n",
    "            col_names.append(f\"cat_col_{i}\")\n",
    "            cat_col_names.append(f\"cat_col_{i}\")\n",
    "        if i in num_cols:\n",
    "            col_names.append(f\"num_col_{i}\")\n",
    "            num_col_names.append(f\"num_col_{i}\")\n",
    "    X = pd.DataFrame(X, columns=col_names)\n",
    "    y = pd.Series(y, name=\"target\")\n",
    "    data = X.join(y)\n",
    "    return data, cat_col_names, num_col_names\n",
    "    \n",
    "def load_classification_data():\n",
    "    dataset = fetch_covtype(data_home=\"data\")\n",
    "    data = np.hstack([dataset.data, dataset.target.reshape(-1, 1)])\n",
    "    col_names = [f\"feature_{i}\" for i in range(data.shape[-1])]\n",
    "    col_names[-1] = \"target\"\n",
    "    data = pd.DataFrame(data, columns=col_names)\n",
    "    data[\"feature_0_cat\"] = pd.qcut(data[\"feature_0\"], q=4)\n",
    "    data[\"feature_0_cat\"] = \"feature_0_\" + data.feature_0_cat.cat.codes.astype(str)\n",
    "    test_idx = data.sample(int(0.2 * len(data)), random_state=42).index\n",
    "    test = data[data.index.isin(test_idx)]\n",
    "    train = data[~data.index.isin(test_idx)]\n",
    "    return (train, test, [\"target\"])\n",
    "\n",
    "def print_metrics(y_true, y_pred, tag):\n",
    "    if isinstance(y_true, pd.DataFrame) or isinstance(y_true, pd.Series):\n",
    "        y_true = y_true.values\n",
    "    if isinstance(y_pred, pd.DataFrame) or isinstance(y_pred, pd.Series):\n",
    "        y_pred = y_pred.values\n",
    "    if y_true.ndim>1:\n",
    "        y_true=y_true.ravel()\n",
    "    if y_pred.ndim>1:\n",
    "        y_pred=y_pred.ravel()\n",
    "    val_acc = accuracy_score(y_true, y_pred)\n",
    "    val_f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{tag} Acc: {val_acc} | {tag} F1: {val_f1}\")"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Synthetic Data \n",
    "\n",
    "First of all, let's create a synthetic data which is a mix of numerical and categorical features"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data, cat_col_names, num_col_names = make_mixed_classification(n_samples=10000, n_features=20, n_categories=4)\n",
    "train, test = train_test_split(data, random_state=42)\n",
    "train, val = train_test_split(train, random_state=42)"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline\n",
    "\n",
    "Let's use the default LightGBM model as a baseline."
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "clf = lgb.LGBMClassifier(random_state=42)\n",
    "clf.fit(train.drop(columns='target'), train['target'], categorical_feature=cat_col_names)\n",
    "val_pred = clf.predict(val.drop(columns='target'))\n",
    "print_metrics(val['target'], val_pred, \"Validation\")\n",
    "test_pred = clf.predict(test.drop(columns='target'))\n",
    "print_metrics(test['target'], test_pred, \"Holdout\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/lightgbm/basic.py:1702: UserWarning: Using categorical_feature in Dataset.\n",
      "  _log_warning('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Acc: 0.9328 | Validation F1: 0.9322580645161291\n",
      "Holdout Acc: 0.9328 | Holdout F1: 0.9330677290836654\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing the Library"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, NodeConfig, TabNetModelConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\n",
    "from pytorch_tabular.categorical_encoders import CategoricalEmbeddingTransformer\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the Configs\n",
    "\n",
    "This is the most crucial step in the process. There are four configs that you need to provide(most of them have intelligent default values), which will drive the rest of the process.\n",
    "\n",
    "* DataConfig - Define the target column names, categorical and numerical column names, any transformation you need to do, etc.\n",
    "* ModelConfig - There is a specific config for each of the models. This determines which model we are going to train and also lets you define the hyperparameters of the model\n",
    "* TrainerConfig - This let's you configure the training process by setting things like batch_size, epochs, early stopping, etc. The vast majority of parameters are directly borrowed from PyTorch Lightning and is passed to the underlying Trainer object during training\n",
    "* OptimizerConfig - This let's you define and use different Optimizers and LearningRate Schedulers. Standard PyTorch Optimizers and Learning RateSchedulers are supported. For custom optimizers, you can use the parameter in the fit method to overwrite this. The custom optimizer should be PyTorch compatible\n",
    "* ExperimentConfig - This is an optional parameter. If set, this defines the Experiment Tracking. Right now, only two experiment tracking frameworks are supported: Tensorboard and Weights&Biases. W&B experiment tracker has more features like tracking the gradients and logits across epochs."
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "data_config = DataConfig(\n",
    "    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    "    continuous_feature_transform=\"quantile_normal\",\n",
    "    normalize_continuous_features=True\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=1024,\n",
    "    max_epochs=1000,\n",
    "    auto_select_gpus=False,\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"4096-4096-512\",  # Number of nodes in each layer\n",
    "    activation=\"LeakyReLU\", # Activation between each layers\n",
    "    learning_rate = 1e-3,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model \n",
    "Now that we have defined the configs and the TabularModel. We just need to call the `fit` method and pass the train and test dataframes. We can also pass in validation dataframe. But if omitted, TabularModel will separate 20% of the data as validation."
   ],
   "metadata": {
    "Collapsed": "true"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tabular_model.fit(train=train, test=test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name                   | Type                | Params\n",
      "---------------------------------------------------------------\n",
      "0 | embedding_layers       | ModuleList          | 45    \n",
      "1 | normalizing_batch_norm | BatchNorm1d         | 34    \n",
      "2 | backbone               | FeedForwardBackbone | 19.0 M\n",
      "3 | output_layer           | Linear              | 1.0 K \n",
      "4 | loss                   | CrossEntropyLoss    | 0     \n",
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Finding best initial lr:  88%|████████▊ | 88/100 [04:20<00:35,  2.96s/it]\n",
      "LR finder stopped early due to diverging loss.\n",
      "Learning rate set to 0.0005248074602497723\n",
      "\n",
      "  | Name                   | Type                | Params\n",
      "---------------------------------------------------------------\n",
      "0 | embedding_layers       | ModuleList          | 45    \n",
      "1 | normalizing_batch_norm | BatchNorm1d         | 34    \n",
      "2 | backbone               | FeedForwardBackbone | 19.0 M\n",
      "3 | output_layer           | Linear              | 1.0 K \n",
      "4 | loss                   | CrossEntropyLoss    | 0     \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 32: 100%|██████████| 7/7 [00:35<00:00,  7.09s/it, loss=0.552, train_loss=0.59, valid_loss=0.423, valid_accuracy=0.831, train_accuracy=0.698]"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating the Model\n",
    "To evaluate the model on new data on the same metrics/loss that was used during training, we can use the `evaluate` method"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "result = tabular_model.evaluate(test)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 3/3 [00:15<00:00,  4.14s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': tensor(0.8416),\n",
      " 'train_accuracy': tensor(0.7048),\n",
      " 'train_loss': tensor(0.5903),\n",
      " 'valid_accuracy': tensor(0.8311),\n",
      " 'valid_loss': tensor(0.4232)}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 3/3 [00:15<00:00,  5.22s/it]\n",
      "[{'train_loss': 0.5903493762016296, 'valid_loss': 0.4231547713279724, 'valid_accuracy': 0.8311111330986023, 'train_accuracy': 0.7048248052597046, 'test_accuracy': 0.8416000008583069}]\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get the prediction as a dataframe, we can use the `predict` method. This will add predictions to the same dataframe that was passed in. For classification problems, we get both the probabilities and the final prediction taking 0.5 as the threshold"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "pred_df = tabular_model.predict(test)\n",
    "pred_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generating Predictions...: 100%|██████████| 3/3 [00:14<00:00,  4.92s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      num_col_0  cat_col_1  num_col_2  num_col_3  num_col_4  num_col_5  \\\n",
       "6252  -2.790932        0.0  -2.010758   3.205420  -0.356361  -0.744417   \n",
       "4684  -0.139585        0.0  -1.207160   2.690514   1.072764  -3.499028   \n",
       "1731   0.001421        1.0  -0.279572   0.363639   0.852329   0.089246   \n",
       "4742   0.086662        3.0   0.798527   0.916448  -1.085978   0.512223   \n",
       "4521   0.982186        2.0  -0.117476  -0.168583  -0.088413  -0.206658   \n",
       "\n",
       "      num_col_6  num_col_7  cat_col_8  num_col_9  ...  num_col_14  num_col_15  \\\n",
       "6252   0.427836  -1.492040        0.0   1.364186  ...   -0.660336   -0.705788   \n",
       "4684   1.561682   0.953991        2.0   1.243788  ...   -2.726836    0.944248   \n",
       "1731   0.084824   0.194984        0.0   2.668561  ...   -0.508633    0.508788   \n",
       "4742  -0.903704   1.538725        2.0   1.518521  ...    0.326685    1.343219   \n",
       "4521  -1.233511  -0.137569        3.0  -1.678887  ...   -0.282845    0.458761   \n",
       "\n",
       "      num_col_16  num_col_17  num_col_18  num_col_19  target  0_probability  \\\n",
       "6252    0.229519    0.060878   -0.464394    2.879481       0       0.145202   \n",
       "4684    0.821184    0.368647   -1.199147    0.126323       1       0.517947   \n",
       "1731   -0.097083   -0.128070   -0.282642   -0.190155       0       0.830036   \n",
       "4742   -1.147619    1.795053    0.857619    0.532915       1       0.469329   \n",
       "4521    1.381926   -0.566849   -0.475947   -0.400418       1       0.269307   \n",
       "\n",
       "      1_probability  prediction  \n",
       "6252       0.854798           1  \n",
       "4684       0.482053           0  \n",
       "1731       0.169964           0  \n",
       "4742       0.530671           1  \n",
       "4521       0.730693           1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_col_0</th>\n",
       "      <th>cat_col_1</th>\n",
       "      <th>num_col_2</th>\n",
       "      <th>num_col_3</th>\n",
       "      <th>num_col_4</th>\n",
       "      <th>num_col_5</th>\n",
       "      <th>num_col_6</th>\n",
       "      <th>num_col_7</th>\n",
       "      <th>cat_col_8</th>\n",
       "      <th>num_col_9</th>\n",
       "      <th>...</th>\n",
       "      <th>num_col_14</th>\n",
       "      <th>num_col_15</th>\n",
       "      <th>num_col_16</th>\n",
       "      <th>num_col_17</th>\n",
       "      <th>num_col_18</th>\n",
       "      <th>num_col_19</th>\n",
       "      <th>target</th>\n",
       "      <th>0_probability</th>\n",
       "      <th>1_probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-2.790932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.010758</td>\n",
       "      <td>3.205420</td>\n",
       "      <td>-0.356361</td>\n",
       "      <td>-0.744417</td>\n",
       "      <td>0.427836</td>\n",
       "      <td>-1.492040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.364186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.660336</td>\n",
       "      <td>-0.705788</td>\n",
       "      <td>0.229519</td>\n",
       "      <td>0.060878</td>\n",
       "      <td>-0.464394</td>\n",
       "      <td>2.879481</td>\n",
       "      <td>0</td>\n",
       "      <td>0.145202</td>\n",
       "      <td>0.854798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>-0.139585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.207160</td>\n",
       "      <td>2.690514</td>\n",
       "      <td>1.072764</td>\n",
       "      <td>-3.499028</td>\n",
       "      <td>1.561682</td>\n",
       "      <td>0.953991</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.243788</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.726836</td>\n",
       "      <td>0.944248</td>\n",
       "      <td>0.821184</td>\n",
       "      <td>0.368647</td>\n",
       "      <td>-1.199147</td>\n",
       "      <td>0.126323</td>\n",
       "      <td>1</td>\n",
       "      <td>0.517947</td>\n",
       "      <td>0.482053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>0.001421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.279572</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.852329</td>\n",
       "      <td>0.089246</td>\n",
       "      <td>0.084824</td>\n",
       "      <td>0.194984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.668561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.508633</td>\n",
       "      <td>0.508788</td>\n",
       "      <td>-0.097083</td>\n",
       "      <td>-0.128070</td>\n",
       "      <td>-0.282642</td>\n",
       "      <td>-0.190155</td>\n",
       "      <td>0</td>\n",
       "      <td>0.830036</td>\n",
       "      <td>0.169964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>0.086662</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.798527</td>\n",
       "      <td>0.916448</td>\n",
       "      <td>-1.085978</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>-0.903704</td>\n",
       "      <td>1.538725</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.518521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326685</td>\n",
       "      <td>1.343219</td>\n",
       "      <td>-1.147619</td>\n",
       "      <td>1.795053</td>\n",
       "      <td>0.857619</td>\n",
       "      <td>0.532915</td>\n",
       "      <td>1</td>\n",
       "      <td>0.469329</td>\n",
       "      <td>0.530671</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>0.982186</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.117476</td>\n",
       "      <td>-0.168583</td>\n",
       "      <td>-0.088413</td>\n",
       "      <td>-0.206658</td>\n",
       "      <td>-1.233511</td>\n",
       "      <td>-0.137569</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.678887</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282845</td>\n",
       "      <td>0.458761</td>\n",
       "      <td>1.381926</td>\n",
       "      <td>-0.566849</td>\n",
       "      <td>-0.475947</td>\n",
       "      <td>-0.400418</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269307</td>\n",
       "      <td>0.730693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "print_metrics(test['target'], pred_df[\"prediction\"], tag=\"Holdout\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Holdout Acc: 0.8416 | Holdout F1: 0.8558951965065502\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract the Learned Embedding\n",
    "\n",
    "For the models that support (CategoryEmbeddingModel and CategoryEmbeddingNODE), we can extract the learned embeddings into a sci-kit learn style Transformer."
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "transformer = CategoricalEmbeddingTransformer(tabular_model)\n",
    "transf_train = transformer.fit_transform(train)\n",
    "clf = lgb.LGBMClassifier(random_state=42)\n",
    "clf.fit(transf_train.drop(columns='target'), transf_train['target'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Encoding the data...: 100%|██████████| 3/3 [00:00<00:00, 32.60it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "transf_val = transformer.transform(val)\n",
    "val_pred = clf.predict(transf_val.drop(columns='target'))\n",
    "print_metrics(transf_val['target'], val_pred, \"Validation\")\n",
    "transf_test = transformer.transform(test)\n",
    "test_pred = clf.predict(transf_test.drop(columns='target'))\n",
    "print_metrics(transf_test['target'], test_pred, \"Holdout\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Encoding the data...: 100%|██████████| 3/3 [00:00<00:00, 56.56it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation Acc: 0.9264 | Validation F1: 0.925646551724138\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Encoding the data...: 100%|██████████| 3/3 [00:00<00:00, 49.19it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Holdout Acc: 0.934 | Holdout F1: 0.934445768772348\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NODE"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "OptimizerConfig?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mOptimizerConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptimizer_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler_params\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlr_scheduler_monitor_metric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Optimizer and Learning Rate Scheduler configuration.\n",
      "Args:\n",
      "    optimizer (str): Any of the standard optimizers from \n",
      "        [torch.optim](https://pytorch.org/docs/stable/optim.html#algorithms). Defaults to `Adam`\"\n",
      "\n",
      "    optimizer_params (dict): The parameters for the optimizer. If left blank, will use default parameters.\n",
      "\n",
      "    lr_scheduler (Optional[str, NoneType]): The name of the LearningRateScheduler to use, if any, from [torch.optim.lr_scheduler](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate). \n",
      "        If None, will not use any scheduler. Defaults to `None`\n",
      "\n",
      "    lr_scheduler_params (Optional[dict, NoneType]): The parameters for the LearningRateScheduler. If left blank, will use default parameters.\n",
      "\n",
      "    lr_scheduler_monitor_metric (Optional[str, NoneType]): Used with `ReduceLROnPlateau`, where the plateau is decided based on this metric\n",
      "\u001b[0;31mFile:\u001b[0m           ~/GitHub/pytorch_tabular/pytorch_tabular/config/config.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "data_config = DataConfig(\n",
    "    target=['target'],\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    "    continuous_feature_transform=\"quantile_normal\",#\"yeo-johnson\",\n",
    "    normalize_continuous_features=True\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    batch_size=128,\n",
    "    max_epochs=100,\n",
    "    auto_select_gpus=False,\n",
    "    # track_grad_norm=2,\n",
    "    gradient_clip_val=10,\n",
    ")\n",
    "# experiment_config = ExperimentConfig(project_name=\"Tabular_test\", log_logits=True)\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = NodeConfig(\n",
    "    task=\"classification\",\n",
    "    num_layers=2,\n",
    "    num_trees=1024,\n",
    "    learning_rate=1,\n",
    "    embed_categorical=False,\n",
    "    metrics=[\"accuracy\",\"f1\"],\n",
    "    # target_range=(train['block_0'].min().item(), train['block_0'].max().item())\n",
    ")\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "tabular_model.fit(train=train, test=test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n",
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Checkpoint directory saved_models exists and is not empty. With save_top_k=1, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name            | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0 | backbone        | NODEBackbone     | 32.4 M\n",
      "1 | output_response | Lambda           | 0     \n",
      "2 | loss            | CrossEntropyLoss | 0     \n",
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Finding best initial lr: 100%|██████████| 100/100 [56:45<00:00, 25.49s/it]Learning rate set to 0.2754228703338169\n",
      "\n",
      "  | Name            | Type             | Params\n",
      "-----------------------------------------------------\n",
      "0 | backbone        | NODEBackbone     | 32.4 M\n",
      "1 | output_response | Lambda           | 0     \n",
      "2 | loss            | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 19: 100%|██████████| 45/45 [18:35<00:00, 61.95s/it, loss=0.394, train_loss=0.232, valid_loss=0.448, valid_accuracy=0.795, valid_f1=0.795, train_accuracy=0.818, train_f1=0.818]"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false",
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "result = tabular_model.evaluate(test)\n",
    "print(result)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/fonnesbeck/anaconda3/envs/pitch_effect/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 20/20 [07:23<00:00, 22.04s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': tensor(0.8384),\n",
      " 'test_f1': tensor(0.8384),\n",
      " 'train_accuracy': tensor(0.8136),\n",
      " 'train_f1': tensor(0.8136),\n",
      " 'train_loss': tensor(0.2316),\n",
      " 'valid_accuracy': tensor(0.7947),\n",
      " 'valid_f1': tensor(0.7947),\n",
      " 'valid_loss': tensor(0.4481)}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 20/20 [07:23<00:00, 22.18s/it]\n",
      "[{'train_loss': 0.231593519449234, 'valid_loss': 0.44811415672302246, 'valid_accuracy': 0.7946666479110718, 'valid_f1': 0.7946666479110718, 'train_accuracy': 0.8135850429534912, 'train_f1': 0.8135850429534912, 'test_accuracy': 0.8384000062942505, 'test_f1': 0.8384000062942505}]\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "node_pred_df = tabular_model.predict(test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generating Predictions...: 100%|██████████| 20/20 [07:17<00:00, 21.85s/it]\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "print_metrics(test['target'], node_pred_df[\"prediction\"], tag=\"Holdout\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Holdout Acc: 0.8384 | Holdout F1: 0.8382706164931946\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NODE (Cat Embed)"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_config = DataConfig(\n",
    "    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    "    continuous_feature_transform=\"quantile_normal\",\n",
    "    normalize_continuous_features=True\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=True,\n",
    "    batch_size=64,\n",
    "    max_epochs=1000,\n",
    "    # track_grad_norm=2,\n",
    "#     gradient_clip_val=10,\n",
    ")\n",
    "# experiment_config = ExperimentConfig(project_name=\"Tabular_test\", log_logits=True)\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = NodeConfig(\n",
    "    task=\"classification\",\n",
    "    num_layers=2,\n",
    "    num_trees=512,\n",
    "    learning_rate=1,\n",
    "    embed_categorical=True,\n",
    "    # metrics=[\"MeanSquaredLogError\"],\n",
    "    # target_range=(train['block_0'].min().item(), train['block_0'].max().item())\n",
    ")\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tabular_model.fit(train=train, test=test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\miniconda3\\envs\\df_encoder\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Checkpoint directory saved_models exists and is not empty. With save_top_k=1, all files in this directory will be deleted when a checkpoint is saved!\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: False\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "D:\\miniconda3\\envs\\df_encoder\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "GPU available: True, used: True\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | embedding_layers  | ModuleList       | 60    \n",
      "1 | embedding_dropout | Dropout          | 0     \n",
      "2 | dense_block       | DenseODSTBlock   | 3.5 M \n",
      "3 | output_response   | Lambda           | 0     \n",
      "4 | loss              | CrossEntropyLoss | 0     \n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | embedding_layers  | ModuleList       | 60    \n",
      "1 | embedding_dropout | Dropout          | 0     \n",
      "2 | dense_block       | DenseODSTBlock   | 3.5 M \n",
      "3 | output_response   | Lambda           | 0     \n",
      "4 | loss              | CrossEntropyLoss | 0     \n",
      "Finding best initial lr: 100%|███████████████████████████████████████████████████████| 100/100 [00:05<00:00, 17.51it/s]Learning rate set to 0.036307805477010104\n",
      "Learning rate set to 0.036307805477010104\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | embedding_layers  | ModuleList       | 60    \n",
      "1 | embedding_dropout | Dropout          | 0     \n",
      "2 | dense_block       | DenseODSTBlock   | 3.5 M \n",
      "3 | output_response   | Lambda           | 0     \n",
      "4 | loss              | CrossEntropyLoss | 0     \n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | embedding_layers  | ModuleList       | 60    \n",
      "1 | embedding_dropout | Dropout          | 0     \n",
      "2 | dense_block       | DenseODSTBlock   | 3.5 M \n",
      "3 | output_response   | Lambda           | 0     \n",
      "4 | loss              | CrossEntropyLoss | 0     \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1:  80%|▊| 71/89 [00:04<00:01, 10.39it/s, loss=0.655, train_loss=0.63, valid_loss=0.69, valid_accuracy=0.485, tra\n",
      "Epoch 1:  84%|▊| 75/89 [00:04<00:01, 11.15it/s, loss=0.655, train_loss=0.63, valid_loss=0.69, valid_accuracy=0.485, tra\n",
      "Epoch 1:  90%|▉| 80/89 [00:04<00:00, 11.99it/s, loss=0.655, train_loss=0.63, valid_loss=0.69, valid_accuracy=0.485, tra\u001b[A\n",
      "Epoch 1:  96%|▉| 85/89 [00:04<00:00, 12.78it/s, loss=0.655, train_loss=0.63, valid_loss=0.69, valid_accuracy=0.485, tra\u001b[A\n",
      "Epoch 1: 100%|█| 89/89 [00:04<00:00, 12.39it/s, loss=0.655, train_loss=0.63, valid_loss=0.731, valid_accuracy=0.486, tr\u001b[A\n",
      "Epoch 2:  80%|▊| 71/89 [00:04<00:01, 10.44it/s, loss=0.631, train_loss=0.603, valid_loss=0.731, valid_accuracy=0.486, t\u001b[A\n",
      "Epoch 2:  84%|▊| 75/89 [00:04<00:01, 11.20it/s, loss=0.631, train_loss=0.603, valid_loss=0.731, valid_accuracy=0.486, t\n",
      "Epoch 2:  90%|▉| 80/89 [00:04<00:00, 12.04it/s, loss=0.631, train_loss=0.603, valid_loss=0.731, valid_accuracy=0.486, t\u001b[A\n",
      "Epoch 2:  96%|▉| 85/89 [00:04<00:00, 12.83it/s, loss=0.631, train_loss=0.603, valid_loss=0.731, valid_accuracy=0.486, t\u001b[A\n",
      "Epoch 2: 100%|█| 89/89 [00:04<00:00, 12.46it/s, loss=0.631, train_loss=0.603, valid_loss=0.668, valid_accuracy=0.588, t\u001b[A\n",
      "Epoch 3:  76%|▊| 68/89 [00:03<00:02, 10.03it/s, loss=0.626, train_loss=0.673, valid_loss=0.668, valid_accuracy=0.588, t\u001b[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Finding best initial lr: 100%|███████████████████████████████████████████████████████| 100/100 [00:20<00:00, 17.51it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3:  80%|▊| 71/89 [00:04<00:01, 10.38it/s, loss=0.635, train_loss=0.833, valid_loss=0.668, valid_accuracy=0.588, t\n",
      "Epoch 3:  84%|▊| 75/89 [00:04<00:01, 11.14it/s, loss=0.635, train_loss=0.833, valid_loss=0.668, valid_accuracy=0.588, t\n",
      "Epoch 3:  90%|▉| 80/89 [00:04<00:00, 11.98it/s, loss=0.635, train_loss=0.833, valid_loss=0.668, valid_accuracy=0.588, t\u001b[A\n",
      "Epoch 3:  96%|▉| 85/89 [00:04<00:00, 12.77it/s, loss=0.635, train_loss=0.833, valid_loss=0.668, valid_accuracy=0.588, t\u001b[A\n",
      "Epoch 3: 100%|█| 89/89 [00:04<00:00, 13.30it/s, loss=0.635, train_loss=0.833, valid_loss=0.798, valid_accuracy=0.548, t\u001b[A\n",
      "Epoch 4:  80%|▊| 71/89 [00:04<00:01, 10.35it/s, loss=0.606, train_loss=0.516, valid_loss=0.798, valid_accuracy=0.548, t\u001b[A\n",
      "Epoch 4:  84%|▊| 75/89 [00:04<00:01, 11.10it/s, loss=0.606, train_loss=0.516, valid_loss=0.798, valid_accuracy=0.548, t\n",
      "Epoch 4:  90%|▉| 80/89 [00:04<00:00, 11.94it/s, loss=0.606, train_loss=0.516, valid_loss=0.798, valid_accuracy=0.548, t\u001b[A\n",
      "Epoch 4:  96%|▉| 85/89 [00:04<00:00, 12.73it/s, loss=0.606, train_loss=0.516, valid_loss=0.798, valid_accuracy=0.548, t\u001b[A\n",
      "Epoch 4: 100%|█| 89/89 [00:04<00:00, 13.25it/s, loss=0.606, train_loss=0.516, valid_loss=0.69, valid_accuracy=0.524, tr\u001b[A\n",
      "Epoch 5:  80%|▊| 71/89 [00:04<00:01, 10.39it/s, loss=0.629, train_loss=0.681, valid_loss=0.69, valid_accuracy=0.524, tr\u001b[A\n",
      "Epoch 5:  84%|▊| 75/89 [00:04<00:01, 11.14it/s, loss=0.629, train_loss=0.681, valid_loss=0.69, valid_accuracy=0.524, tr\n",
      "Validating:  22%|███████████████▊                                                       | 4/18 [00:00<00:00, 38.94it/s]\u001b[A\n",
      "Epoch 5:  90%|▉| 80/89 [00:04<00:00, 11.96it/s, loss=0.629, train_loss=0.681, valid_loss=0.69, valid_accuracy=0.524, tr\u001b[A\n",
      "Epoch 5:  96%|▉| 85/89 [00:04<00:00, 12.75it/s, loss=0.629, train_loss=0.681, valid_loss=0.69, valid_accuracy=0.524, tr\u001b[A\n",
      "Epoch 5: 100%|█| 89/89 [00:04<00:00, 12.30it/s, loss=0.629, train_loss=0.681, valid_loss=0.668, valid_accuracy=0.548, t\u001b[A\n",
      "Epoch 5: 100%|█| 89/89 [00:14<00:00,  4.12it/s, loss=0.629, train_loss=0.681, valid_loss=0.668, valid_accuracy=0.548, t\u001b[A"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false",
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = tabular_model.evaluate(test)\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_embed_node_pred_df = tabular_model.predict(test)"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print_metrics(test['target'], cat_embed_node_pred_df[\"prediction\"], tag=\"Holdout\")"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use Category embedding"
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "transformer = CategoricalEmbeddingTransformer(tabular_model)\n",
    "transf_train = transformer.fit_transform(train)\n",
    "clf = lgb.LGBMClassifier(random_state=42)\n",
    "clf.fit(transf_train.drop(columns='target'), transf_train['target'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=42)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "transf_val = transformer.transform(val)\n",
    "val_pred = clf.predict(transf_val.drop(columns='target'))\n",
    "val_acc = accuracy_score(transf_val['target'].values.ravel(), val_pred)\n",
    "val_f1 = f1_score(transf_val['target'].values.ravel(), val_pred)\n",
    "print(f\"Val Acc: {val_acc} | Val F1: {val_f1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Val Acc: 0.9322666666666667 | Val F1: 0.932410856838744\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "transf_test = transformer.transform(test)\n",
    "test_pred = clf.predict(transf_test.drop(columns='target'))\n",
    "test_acc = accuracy_score(transf_test['target'].values.ravel(), test_pred)\n",
    "test_f1 = f1_score(transf_test['target'].values.ravel(), test_pred)\n",
    "print(f\"Test Acc: {test_acc} | Test F1: {test_f1}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Acc: 0.9368 | Test F1: 0.9383294301327089\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "## TabNet"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "data_config = DataConfig(\n",
    "    target=['target'],\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    "    continuous_feature_transform=None,#\"yeo-johnson\",\n",
    "    normalize_continuous_features=True\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False,\n",
    "    batch_size=1024,\n",
    "    max_epochs=1000,\n",
    "    # track_grad_norm=2,\n",
    "    gradient_clip_val=10,\n",
    ")\n",
    "# experiment_config = ExperimentConfig(project_name=\"Tabular_test\", log_logits=True)\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "model_config = TabNetModelConfig(\n",
    "    task=\"classification\",\n",
    "    n_d=5,\n",
    "    n_a=5,\n",
    "    n_steps=2,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "7/7 [00:00<00:00, 19.07it/s, loss=0.613, train_loss=0.605, valid_loss=0.612, valid_accuracy=0.676, train_accuracy=0.672]\n",
      "Epoch 20:  71%|███████▏  | 5/7 [00:00<00:00, 16.99it/s, loss=0.608, train_loss=0.634, valid_loss=0.612, valid_accuracy=0.676, train_accuracy=0.67]\n",
      "Epoch 20: 100%|██████████| 7/7 [00:00<00:00, 19.12it/s, loss=0.608, train_loss=0.634, valid_loss=0.604, valid_accuracy=0.687, train_accuracy=0.67]\n",
      "Epoch 21:  71%|███████▏  | 5/7 [00:00<00:00, 17.00it/s, loss=0.599, train_loss=0.582, valid_loss=0.604, valid_accuracy=0.687, train_accuracy=0.68]\n",
      "Epoch 21: 100%|██████████| 7/7 [00:00<00:00, 19.29it/s, loss=0.599, train_loss=0.582, valid_loss=0.596, valid_accuracy=0.697, train_accuracy=0.68]\n",
      "Epoch 22:  71%|███████▏  | 5/7 [00:00<00:00, 16.82it/s, loss=0.592, train_loss=0.587, valid_loss=0.596, valid_accuracy=0.697, train_accuracy=0.696]\n",
      "Epoch 22: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.592, train_loss=0.587, valid_loss=0.589, valid_accuracy=0.708, train_accuracy=0.696]\n",
      "Epoch 23:  71%|███████▏  | 5/7 [00:00<00:00, 17.05it/s, loss=0.583, train_loss=0.558, valid_loss=0.589, valid_accuracy=0.708, train_accuracy=0.698]\n",
      "Epoch 23: 100%|██████████| 7/7 [00:00<00:00, 19.34it/s, loss=0.583, train_loss=0.558, valid_loss=0.578, valid_accuracy=0.709, train_accuracy=0.698]\n",
      "Epoch 24:  71%|███████▏  | 5/7 [00:00<00:00, 15.38it/s, loss=0.575, train_loss=0.569, valid_loss=0.578, valid_accuracy=0.709, train_accuracy=0.711]\n",
      "Epoch 24: 100%|██████████| 7/7 [00:00<00:00, 17.55it/s, loss=0.575, train_loss=0.569, valid_loss=0.57, valid_accuracy=0.719, train_accuracy=0.711] \n",
      "Epoch 25:  71%|███████▏  | 5/7 [00:00<00:00, 17.23it/s, loss=0.566, train_loss=0.533, valid_loss=0.57, valid_accuracy=0.719, train_accuracy=0.715]\n",
      "Epoch 25: 100%|██████████| 7/7 [00:00<00:00, 19.39it/s, loss=0.566, train_loss=0.533, valid_loss=0.558, valid_accuracy=0.726, train_accuracy=0.715]\n",
      "Epoch 26:  71%|███████▏  | 5/7 [00:00<00:00, 16.88it/s, loss=0.560, train_loss=0.566, valid_loss=0.558, valid_accuracy=0.726, train_accuracy=0.73]\n",
      "Epoch 4: 100%|██████████| 7/7 [7:30:40<00:00, 9013.52s/it, loss=0.822, train_loss=0.795, valid_loss=0.761, valid_accuracy=0.529, train_accuracy=0.524]\n",
      "Finding best initial lr: 100%|██████████| 100/100 [01:20<00:00,  1.23it/s]\n",
      "Epoch 4: 100%|██████████| 7/7 [01:12<00:00, 24.29s/it, loss=0.748, train_loss=0.736, valid_loss=0.703, valid_accuracy=0.525, train_accuracy=0.489]\n",
      "Epoch 26: 100%|██████████| 7/7 [00:00<00:00, 15.09it/s, loss=0.560, train_loss=0.566, valid_loss=0.547, valid_accuracy=0.741, train_accuracy=0.73]\n",
      "Epoch 27:  29%|██▊       | 2/7 [00:00<00:00, 13.37it/s, loss=0.557, train_loss=0.548, valid_loss=0.547, valid_accuracy=0.741, train_accuracy=0.729]\n",
      "Epoch 27:  71%|███████▏  | 5/7 [00:00<00:00, 16.88it/s, loss=0.552, train_loss=0.521, valid_loss=0.547, valid_accuracy=0.741, train_accuracy=0.729]\n",
      "Epoch 27: 100%|██████████| 7/7 [00:00<00:00, 19.02it/s, loss=0.552, train_loss=0.521, valid_loss=0.536, valid_accuracy=0.752, train_accuracy=0.729]\n",
      "Epoch 28:  71%|███████▏  | 5/7 [00:00<00:00, 16.17it/s, loss=0.543, train_loss=0.52, valid_loss=0.536, valid_accuracy=0.752, train_accuracy=0.734] \n",
      "Epoch 28: 100%|██████████| 7/7 [00:00<00:00, 18.00it/s, loss=0.543, train_loss=0.52, valid_loss=0.526, valid_accuracy=0.76, train_accuracy=0.734] \n",
      "Epoch 29:  71%|███████▏  | 5/7 [00:00<00:00, 16.88it/s, loss=0.534, train_loss=0.534, valid_loss=0.526, valid_accuracy=0.76, train_accuracy=0.747]\n",
      "Epoch 29: 100%|██████████| 7/7 [00:00<00:00, 19.07it/s, loss=0.534, train_loss=0.534, valid_loss=0.516, valid_accuracy=0.771, train_accuracy=0.747]\n",
      "Epoch 30:  71%|███████▏  | 5/7 [00:00<00:00, 15.92it/s, loss=0.524, train_loss=0.52, valid_loss=0.516, valid_accuracy=0.771, train_accuracy=0.756] \n",
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 18.04it/s, loss=0.524, train_loss=0.52, valid_loss=0.505, valid_accuracy=0.78, train_accuracy=0.756] \n",
      "Epoch 31:  71%|███████▏  | 5/7 [00:00<00:00, 16.71it/s, loss=0.514, train_loss=0.505, valid_loss=0.505, valid_accuracy=0.78, train_accuracy=0.759]\n",
      "Epoch 31: 100%|██████████| 7/7 [00:00<00:00, 18.47it/s, loss=0.514, train_loss=0.505, valid_loss=0.495, valid_accuracy=0.793, train_accuracy=0.759]\n",
      "Epoch 32:  71%|███████▏  | 5/7 [00:00<00:00, 16.44it/s, loss=0.505, train_loss=0.51, valid_loss=0.495, valid_accuracy=0.793, train_accuracy=0.77] \n",
      "Epoch 32: 100%|██████████| 7/7 [00:00<00:00, 18.52it/s, loss=0.505, train_loss=0.51, valid_loss=0.485, valid_accuracy=0.796, train_accuracy=0.77]\n",
      "Epoch 33:  71%|███████▏  | 5/7 [00:00<00:00, 15.72it/s, loss=0.495, train_loss=0.477, valid_loss=0.485, valid_accuracy=0.796, train_accuracy=0.772]\n",
      "Epoch 33: 100%|██████████| 7/7 [00:00<00:00, 17.77it/s, loss=0.495, train_loss=0.477, valid_loss=0.475, valid_accuracy=0.797, train_accuracy=0.772]\n",
      "Epoch 34:  71%|███████▏  | 5/7 [00:00<00:00, 16.82it/s, loss=0.487, train_loss=0.482, valid_loss=0.475, valid_accuracy=0.797, train_accuracy=0.784]\n",
      "Epoch 34: 100%|██████████| 7/7 [00:00<00:00, 18.67it/s, loss=0.487, train_loss=0.482, valid_loss=0.465, valid_accuracy=0.805, train_accuracy=0.784]\n",
      "Epoch 35:  71%|███████▏  | 5/7 [00:00<00:00, 15.82it/s, loss=0.479, train_loss=0.486, valid_loss=0.465, valid_accuracy=0.805, train_accuracy=0.791]\n",
      "Epoch 35: 100%|██████████| 7/7 [00:00<00:00, 17.95it/s, loss=0.479, train_loss=0.486, valid_loss=0.456, valid_accuracy=0.806, train_accuracy=0.791]\n",
      "Epoch 36:  71%|███████▏  | 5/7 [00:00<00:00, 16.07it/s, loss=0.469, train_loss=0.468, valid_loss=0.456, valid_accuracy=0.806, train_accuracy=0.799]\n",
      "Epoch 36: 100%|██████████| 7/7 [00:00<00:00, 18.33it/s, loss=0.469, train_loss=0.468, valid_loss=0.448, valid_accuracy=0.813, train_accuracy=0.799]\n",
      "Epoch 37:  71%|███████▏  | 5/7 [00:00<00:00, 16.17it/s, loss=0.462, train_loss=0.472, valid_loss=0.448, valid_accuracy=0.813, train_accuracy=0.797]\n",
      "Epoch 37: 100%|██████████| 7/7 [00:00<00:00, 18.47it/s, loss=0.462, train_loss=0.472, valid_loss=0.441, valid_accuracy=0.823, train_accuracy=0.797]\n",
      "Epoch 38:  71%|███████▏  | 5/7 [00:00<00:00, 16.66it/s, loss=0.454, train_loss=0.434, valid_loss=0.441, valid_accuracy=0.823, train_accuracy=0.801]\n",
      "Epoch 38: 100%|██████████| 7/7 [00:00<00:00, 18.77it/s, loss=0.454, train_loss=0.434, valid_loss=0.434, valid_accuracy=0.828, train_accuracy=0.801]\n",
      "Epoch 39:  71%|███████▏  | 5/7 [00:00<00:00, 16.38it/s, loss=0.447, train_loss=0.485, valid_loss=0.434, valid_accuracy=0.828, train_accuracy=0.807]\n",
      "Epoch 39: 100%|██████████| 7/7 [00:00<00:00, 18.62it/s, loss=0.447, train_loss=0.485, valid_loss=0.428, valid_accuracy=0.829, train_accuracy=0.807]\n",
      "Epoch 40:  71%|███████▏  | 5/7 [00:00<00:00, 16.71it/s, loss=0.442, train_loss=0.435, valid_loss=0.428, valid_accuracy=0.829, train_accuracy=0.811]\n",
      "Epoch 40: 100%|██████████| 7/7 [00:00<00:00, 18.97it/s, loss=0.442, train_loss=0.435, valid_loss=0.422, valid_accuracy=0.831, train_accuracy=0.811]\n",
      "Epoch 41:  71%|███████▏  | 5/7 [00:00<00:00, 15.28it/s, loss=0.434, train_loss=0.389, valid_loss=0.422, valid_accuracy=0.831, train_accuracy=0.814]\n",
      "Epoch 41: 100%|██████████| 7/7 [00:00<00:00, 17.20it/s, loss=0.434, train_loss=0.389, valid_loss=0.417, valid_accuracy=0.838, train_accuracy=0.814]\n",
      "Epoch 42:  71%|███████▏  | 5/7 [00:00<00:00, 16.17it/s, loss=0.427, train_loss=0.399, valid_loss=0.417, valid_accuracy=0.838, train_accuracy=0.823]\n",
      "Epoch 42: 100%|██████████| 7/7 [00:00<00:00, 18.47it/s, loss=0.427, train_loss=0.399, valid_loss=0.411, valid_accuracy=0.84, train_accuracy=0.823] \n",
      "Epoch 43:  71%|███████▏  | 5/7 [00:00<00:00, 16.44it/s, loss=0.418, train_loss=0.382, valid_loss=0.411, valid_accuracy=0.84, train_accuracy=0.829]\n",
      "Epoch 43: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.418, train_loss=0.382, valid_loss=0.406, valid_accuracy=0.845, train_accuracy=0.829]\n",
      "Epoch 44:  71%|███████▏  | 5/7 [00:00<00:00, 16.49it/s, loss=0.412, train_loss=0.414, valid_loss=0.406, valid_accuracy=0.845, train_accuracy=0.832]\n",
      "Epoch 44: 100%|██████████| 7/7 [00:00<00:00, 18.82it/s, loss=0.412, train_loss=0.414, valid_loss=0.402, valid_accuracy=0.848, train_accuracy=0.832]\n",
      "Epoch 45:  71%|███████▏  | 5/7 [00:00<00:00, 15.15it/s, loss=0.410, train_loss=0.439, valid_loss=0.402, valid_accuracy=0.848, train_accuracy=0.837]\n",
      "Epoch 45: 100%|██████████| 7/7 [00:00<00:00, 17.12it/s, loss=0.410, train_loss=0.439, valid_loss=0.399, valid_accuracy=0.846, train_accuracy=0.837]\n",
      "Epoch 46:  71%|███████▏  | 5/7 [00:00<00:00, 16.71it/s, loss=0.407, train_loss=0.432, valid_loss=0.399, valid_accuracy=0.846, train_accuracy=0.832]\n",
      "Epoch 46: 100%|██████████| 7/7 [00:00<00:00, 19.02it/s, loss=0.407, train_loss=0.432, valid_loss=0.396, valid_accuracy=0.851, train_accuracy=0.832]\n",
      "Epoch 47:  71%|███████▏  | 5/7 [00:00<00:00, 16.77it/s, loss=0.402, train_loss=0.377, valid_loss=0.396, valid_accuracy=0.851, train_accuracy=0.836]\n",
      "Epoch 47: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.402, train_loss=0.377, valid_loss=0.392, valid_accuracy=0.857, train_accuracy=0.836]\n",
      "Epoch 48:  71%|███████▏  | 5/7 [00:00<00:00, 16.49it/s, loss=0.397, train_loss=0.375, valid_loss=0.392, valid_accuracy=0.857, train_accuracy=0.843]\n",
      "Epoch 48: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.397, train_loss=0.375, valid_loss=0.389, valid_accuracy=0.861, train_accuracy=0.843]\n",
      "Epoch 49:  71%|███████▏  | 5/7 [00:00<00:00, 16.38it/s, loss=0.393, train_loss=0.414, valid_loss=0.389, valid_accuracy=0.861, train_accuracy=0.846]\n",
      "Epoch 49: 100%|██████████| 7/7 [00:00<00:00, 18.67it/s, loss=0.393, train_loss=0.414, valid_loss=0.387, valid_accuracy=0.861, train_accuracy=0.846]\n",
      "Epoch 50:  71%|███████▏  | 5/7 [00:00<00:00, 16.55it/s, loss=0.389, train_loss=0.386, valid_loss=0.387, valid_accuracy=0.861, train_accuracy=0.842]\n",
      "Epoch 50: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.389, train_loss=0.386, valid_loss=0.384, valid_accuracy=0.868, train_accuracy=0.842]\n",
      "Epoch 51:  71%|███████▏  | 5/7 [00:00<00:00, 15.67it/s, loss=0.387, train_loss=0.377, valid_loss=0.384, valid_accuracy=0.868, train_accuracy=0.844]\n",
      "Epoch 51: 100%|██████████| 7/7 [00:00<00:00, 17.95it/s, loss=0.387, train_loss=0.377, valid_loss=0.381, valid_accuracy=0.868, train_accuracy=0.844]\n",
      "Epoch 52:  71%|███████▏  | 5/7 [00:00<00:00, 16.28it/s, loss=0.383, train_loss=0.347, valid_loss=0.381, valid_accuracy=0.868, train_accuracy=0.849]\n",
      "Epoch 52: 100%|██████████| 7/7 [00:00<00:00, 18.09it/s, loss=0.383, train_loss=0.347, valid_loss=0.38, valid_accuracy=0.866, train_accuracy=0.849] \n",
      "Epoch 53:  71%|███████▏  | 5/7 [00:00<00:00, 16.44it/s, loss=0.379, train_loss=0.404, valid_loss=0.38, valid_accuracy=0.866, train_accuracy=0.849]\n",
      "Epoch 53: 100%|██████████| 7/7 [00:00<00:00, 18.04it/s, loss=0.379, train_loss=0.404, valid_loss=0.378, valid_accuracy=0.869, train_accuracy=0.849]\n",
      "Epoch 54:  71%|███████▏  | 5/7 [00:00<00:00, 16.22it/s, loss=0.376, train_loss=0.358, valid_loss=0.378, valid_accuracy=0.869, train_accuracy=0.846]\n",
      "Epoch 54: 100%|██████████| 7/7 [00:00<00:00, 18.33it/s, loss=0.376, train_loss=0.358, valid_loss=0.375, valid_accuracy=0.871, train_accuracy=0.846]\n",
      "Epoch 55:  71%|███████▏  | 5/7 [00:00<00:00, 17.05it/s, loss=0.373, train_loss=0.369, valid_loss=0.375, valid_accuracy=0.871, train_accuracy=0.851]\n",
      "Epoch 55: 100%|██████████| 7/7 [00:00<00:00, 18.97it/s, loss=0.373, train_loss=0.369, valid_loss=0.373, valid_accuracy=0.87, train_accuracy=0.851] \n",
      "Epoch 56:  71%|███████▏  | 5/7 [00:00<00:00, 16.60it/s, loss=0.371, train_loss=0.359, valid_loss=0.373, valid_accuracy=0.87, train_accuracy=0.855]\n",
      "Epoch 56: 100%|██████████| 7/7 [00:00<00:00, 18.87it/s, loss=0.371, train_loss=0.359, valid_loss=0.371, valid_accuracy=0.87, train_accuracy=0.855]\n",
      "Epoch 57:  71%|███████▏  | 5/7 [00:00<00:00, 16.71it/s, loss=0.369, train_loss=0.375, valid_loss=0.371, valid_accuracy=0.87, train_accuracy=0.86]\n",
      "Epoch 57: 100%|██████████| 7/7 [00:00<00:00, 18.92it/s, loss=0.369, train_loss=0.375, valid_loss=0.369, valid_accuracy=0.873, train_accuracy=0.86]\n",
      "Epoch 58:  71%|███████▏  | 5/7 [00:00<00:00, 16.66it/s, loss=0.366, train_loss=0.348, valid_loss=0.369, valid_accuracy=0.873, train_accuracy=0.854]\n",
      "Epoch 58: 100%|██████████| 7/7 [00:00<00:00, 19.02it/s, loss=0.366, train_loss=0.348, valid_loss=0.366, valid_accuracy=0.873, train_accuracy=0.854]\n",
      "Epoch 59:  71%|███████▏  | 5/7 [00:00<00:00, 16.55it/s, loss=0.366, train_loss=0.414, valid_loss=0.366, valid_accuracy=0.873, train_accuracy=0.856]\n",
      "Epoch 59: 100%|██████████| 7/7 [00:00<00:00, 18.77it/s, loss=0.366, train_loss=0.414, valid_loss=0.364, valid_accuracy=0.874, train_accuracy=0.856]\n",
      "Epoch 60:  71%|███████▏  | 5/7 [00:00<00:00, 14.49it/s, loss=0.364, train_loss=0.336, valid_loss=0.364, valid_accuracy=0.874, train_accuracy=0.851]\n",
      "Epoch 60: 100%|██████████| 7/7 [00:00<00:00, 16.36it/s, loss=0.364, train_loss=0.336, valid_loss=0.363, valid_accuracy=0.873, train_accuracy=0.851]\n",
      "Epoch 61:  71%|███████▏  | 5/7 [00:00<00:00, 14.70it/s, loss=0.361, train_loss=0.359, valid_loss=0.363, valid_accuracy=0.873, train_accuracy=0.857]\n",
      "Epoch 61: 100%|██████████| 7/7 [00:00<00:00, 16.91it/s, loss=0.361, train_loss=0.359, valid_loss=0.361, valid_accuracy=0.877, train_accuracy=0.857]\n",
      "Epoch 62:  71%|███████▏  | 5/7 [00:00<00:00, 16.55it/s, loss=0.358, train_loss=0.379, valid_loss=0.361, valid_accuracy=0.877, train_accuracy=0.856]\n",
      "Epoch 62: 100%|██████████| 7/7 [00:00<00:00, 18.92it/s, loss=0.358, train_loss=0.379, valid_loss=0.359, valid_accuracy=0.877, train_accuracy=0.856]\n",
      "Epoch 63:  71%|███████▏  | 5/7 [00:00<00:00, 16.44it/s, loss=0.354, train_loss=0.348, valid_loss=0.359, valid_accuracy=0.877, train_accuracy=0.855]\n",
      "Epoch 63: 100%|██████████| 7/7 [00:00<00:00, 18.67it/s, loss=0.354, train_loss=0.348, valid_loss=0.356, valid_accuracy=0.876, train_accuracy=0.855]\n",
      "Epoch 64:  71%|███████▏  | 5/7 [00:00<00:00, 16.77it/s, loss=0.353, train_loss=0.348, valid_loss=0.356, valid_accuracy=0.876, train_accuracy=0.859]\n",
      "Epoch 64: 100%|██████████| 7/7 [00:00<00:00, 18.62it/s, loss=0.353, train_loss=0.348, valid_loss=0.354, valid_accuracy=0.876, train_accuracy=0.859]\n",
      "Epoch 65:  71%|███████▏  | 5/7 [00:00<00:00, 16.54it/s, loss=0.353, train_loss=0.408, valid_loss=0.354, valid_accuracy=0.876, train_accuracy=0.858]\n",
      "Epoch 65: 100%|██████████| 7/7 [00:00<00:00, 18.82it/s, loss=0.353, train_loss=0.408, valid_loss=0.353, valid_accuracy=0.876, train_accuracy=0.858]\n",
      "Epoch 66:  71%|███████▏  | 5/7 [00:00<00:00, 16.17it/s, loss=0.352, train_loss=0.358, valid_loss=0.353, valid_accuracy=0.876, train_accuracy=0.856]\n",
      "Epoch 66: 100%|██████████| 7/7 [00:00<00:00, 18.28it/s, loss=0.352, train_loss=0.358, valid_loss=0.352, valid_accuracy=0.879, train_accuracy=0.856]\n",
      "Epoch 67:  71%|███████▏  | 5/7 [00:00<00:00, 16.82it/s, loss=0.350, train_loss=0.333, valid_loss=0.352, valid_accuracy=0.879, train_accuracy=0.86]\n",
      "Epoch 67: 100%|██████████| 7/7 [00:00<00:00, 19.12it/s, loss=0.350, train_loss=0.333, valid_loss=0.35, valid_accuracy=0.878, train_accuracy=0.86] \n",
      "Epoch 68:  71%|███████▏  | 5/7 [00:00<00:00, 16.99it/s, loss=0.347, train_loss=0.325, valid_loss=0.35, valid_accuracy=0.878, train_accuracy=0.861]\n",
      "Epoch 68: 100%|██████████| 7/7 [00:00<00:00, 19.23it/s, loss=0.347, train_loss=0.325, valid_loss=0.349, valid_accuracy=0.878, train_accuracy=0.861]\n",
      "Epoch 69:  71%|███████▏  | 5/7 [00:00<00:00, 16.66it/s, loss=0.346, train_loss=0.398, valid_loss=0.349, valid_accuracy=0.878, train_accuracy=0.864]\n",
      "Epoch 69: 100%|██████████| 7/7 [00:00<00:00, 18.87it/s, loss=0.346, train_loss=0.398, valid_loss=0.348, valid_accuracy=0.878, train_accuracy=0.864]\n",
      "Epoch 70:  71%|███████▏  | 5/7 [00:00<00:00, 16.82it/s, loss=0.343, train_loss=0.316, valid_loss=0.348, valid_accuracy=0.878, train_accuracy=0.864]\n",
      "Epoch 70: 100%|██████████| 7/7 [00:00<00:00, 19.18it/s, loss=0.343, train_loss=0.316, valid_loss=0.347, valid_accuracy=0.878, train_accuracy=0.864]\n",
      "Epoch 71:  71%|███████▏  | 5/7 [00:00<00:00, 16.60it/s, loss=0.343, train_loss=0.376, valid_loss=0.347, valid_accuracy=0.878, train_accuracy=0.866]\n",
      "Epoch 71: 100%|██████████| 7/7 [00:00<00:00, 18.87it/s, loss=0.343, train_loss=0.376, valid_loss=0.346, valid_accuracy=0.879, train_accuracy=0.866]\n",
      "Epoch 72:  71%|███████▏  | 5/7 [00:00<00:00, 16.88it/s, loss=0.342, train_loss=0.328, valid_loss=0.346, valid_accuracy=0.879, train_accuracy=0.863]\n",
      "Epoch 72: 100%|██████████| 7/7 [00:00<00:00, 18.87it/s, loss=0.342, train_loss=0.328, valid_loss=0.346, valid_accuracy=0.88, train_accuracy=0.863] \n",
      "Epoch 73:  71%|███████▏  | 5/7 [00:00<00:00, 17.11it/s, loss=0.339, train_loss=0.378, valid_loss=0.346, valid_accuracy=0.88, train_accuracy=0.863]\n",
      "Epoch 73: 100%|██████████| 7/7 [00:00<00:00, 19.44it/s, loss=0.339, train_loss=0.378, valid_loss=0.345, valid_accuracy=0.878, train_accuracy=0.863]\n",
      "Epoch 74:  71%|███████▏  | 5/7 [00:00<00:00, 15.43it/s, loss=0.337, train_loss=0.321, valid_loss=0.345, valid_accuracy=0.878, train_accuracy=0.866]\n",
      "Epoch 74: 100%|██████████| 7/7 [00:00<00:00, 17.29it/s, loss=0.337, train_loss=0.321, valid_loss=0.343, valid_accuracy=0.879, train_accuracy=0.866]\n",
      "Epoch 75:  71%|███████▏  | 5/7 [00:00<00:00, 16.82it/s, loss=0.336, train_loss=0.343, valid_loss=0.343, valid_accuracy=0.879, train_accuracy=0.869]\n",
      "Epoch 75: 100%|██████████| 7/7 [00:00<00:00, 18.92it/s, loss=0.336, train_loss=0.343, valid_loss=0.343, valid_accuracy=0.878, train_accuracy=0.869]\n",
      "Epoch 76:  71%|███████▏  | 5/7 [00:00<00:00, 16.33it/s, loss=0.335, train_loss=0.319, valid_loss=0.343, valid_accuracy=0.878, train_accuracy=0.867]\n",
      "Epoch 76: 100%|██████████| 7/7 [00:00<00:00, 17.81it/s, loss=0.335, train_loss=0.319, valid_loss=0.342, valid_accuracy=0.877, train_accuracy=0.867]\n",
      "Epoch 77:  71%|███████▏  | 5/7 [00:00<00:00, 16.49it/s, loss=0.334, train_loss=0.376, valid_loss=0.342, valid_accuracy=0.877, train_accuracy=0.864]\n",
      "Epoch 77: 100%|██████████| 7/7 [00:00<00:00, 18.72it/s, loss=0.334, train_loss=0.376, valid_loss=0.342, valid_accuracy=0.877, train_accuracy=0.864]\n",
      "Epoch 78:  71%|███████▏  | 5/7 [00:00<00:00, 16.38it/s, loss=0.334, train_loss=0.339, valid_loss=0.342, valid_accuracy=0.877, train_accuracy=0.866]\n",
      "Epoch 78: 100%|██████████| 7/7 [00:00<00:00, 18.52it/s, loss=0.334, train_loss=0.339, valid_loss=0.34, valid_accuracy=0.877, train_accuracy=0.866] \n",
      "Epoch 79:  71%|███████▏  | 5/7 [00:00<00:00, 17.17it/s, loss=0.335, train_loss=0.385, valid_loss=0.34, valid_accuracy=0.877, train_accuracy=0.865]\n",
      "Epoch 79: 100%|██████████| 7/7 [00:00<00:00, 19.39it/s, loss=0.335, train_loss=0.385, valid_loss=0.339, valid_accuracy=0.878, train_accuracy=0.865]\n",
      "Epoch 80:  71%|███████▏  | 5/7 [00:00<00:00, 14.53it/s, loss=0.335, train_loss=0.361, valid_loss=0.339, valid_accuracy=0.878, train_accuracy=0.861]\n",
      "Epoch 80: 100%|██████████| 7/7 [00:00<00:00, 16.17it/s, loss=0.335, train_loss=0.361, valid_loss=0.338, valid_accuracy=0.877, train_accuracy=0.861]\n",
      "Epoch 81:  71%|███████▏  | 5/7 [00:00<00:00, 13.85it/s, loss=0.332, train_loss=0.3, valid_loss=0.338, valid_accuracy=0.877, train_accuracy=0.863]  \n",
      "Epoch 81: 100%|██████████| 7/7 [00:00<00:00, 16.10it/s, loss=0.332, train_loss=0.3, valid_loss=0.337, valid_accuracy=0.877, train_accuracy=0.863]\n",
      "Epoch 82:  71%|███████▏  | 5/7 [00:00<00:00, 16.44it/s, loss=0.332, train_loss=0.338, valid_loss=0.337, valid_accuracy=0.877, train_accuracy=0.863]\n",
      "Epoch 82: 100%|██████████| 7/7 [00:00<00:00, 19.66it/s, loss=0.332, train_loss=0.338, valid_loss=0.337, valid_accuracy=0.877, train_accuracy=0.863]\n",
      "Epoch 83:  71%|███████▏  | 5/7 [00:00<00:00, 16.88it/s, loss=0.330, train_loss=0.319, valid_loss=0.337, valid_accuracy=0.877, train_accuracy=0.86]\n",
      "Epoch 83: 100%|██████████| 7/7 [00:00<00:00, 19.07it/s, loss=0.330, train_loss=0.319, valid_loss=0.336, valid_accuracy=0.878, train_accuracy=0.86]\n",
      "Epoch 84:  71%|███████▏  | 5/7 [00:00<00:00, 17.01it/s, loss=0.328, train_loss=0.328, valid_loss=0.336, valid_accuracy=0.878, train_accuracy=0.865]\n",
      "Epoch 84: 100%|██████████| 7/7 [00:00<00:00, 19.24it/s, loss=0.328, train_loss=0.328, valid_loss=0.336, valid_accuracy=0.877, train_accuracy=0.865]\n",
      "Epoch 84: 100%|██████████| 7/7 [00:00<00:00, 18.98it/s, loss=0.328, train_loss=0.328, valid_loss=0.336, valid_accuracy=0.877, train_accuracy=0.865]\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false",
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "tabular_model.fit(train=train, test=test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 27.85it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_accuracy': tensor(0.8600, device='cuda:0'),\n",
      " 'train_accuracy': tensor(0.8683, device='cuda:0'),\n",
      " 'train_loss': tensor(0.3278, device='cuda:0'),\n",
      " 'valid_accuracy': tensor(0.8773, device='cuda:0'),\n",
      " 'valid_loss': tensor(0.3365, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 3/3 [00:00<00:00, 23.87it/s]\n",
      "[{'train_loss': 0.3278144598007202, 'valid_loss': 0.3364557921886444, 'valid_accuracy': 0.8773333430290222, 'train_accuracy': 0.8682800531387329, 'test_accuracy': 0.8600000143051147}]\n"
     ]
    }
   ],
   "metadata": {
    "Collapsed": "false"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "result = tabular_model.evaluate(test)\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {
    "Collapsed": "false"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pitch_effect': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "572246634311884dbf575e2c66fbaff9d1d481733d599f410fd23ebcfad6ab4b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
