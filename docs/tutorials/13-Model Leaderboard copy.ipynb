{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_tabular.utils import load_covertype_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data, cat_col_names, num_col_names, target_col = load_covertype_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Importing the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from pytorch_tabular import TabularModel, model_sweep\n",
    "from pytorch_tabular.models import (\n",
    "    CategoryEmbeddingModelConfig,\n",
    "    DANetConfig,\n",
    "    GANDALFConfig,\n",
    "    FTTransformerConfig,\n",
    "    TabNetModelConfig\n",
    ")\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig, ExperimentConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, test \u001b[38;5;241m=\u001b[39m train_test_split(data, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(data, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sweep\n",
    "\n",
    "Define the data config, trainer config, and optimizer config and do a sweep of multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_config \u001b[38;5;241m=\u001b[39m DataConfig(\n\u001b[1;32m      2\u001b[0m     target\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m----> 3\u001b[0m         target_col\n\u001b[1;32m      4\u001b[0m     ],  \u001b[38;5;66;03m# target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     continuous_cols\u001b[38;5;241m=\u001b[39mnum_col_names,\n\u001b[1;32m      6\u001b[0m     categorical_cols\u001b[38;5;241m=\u001b[39mcat_col_names,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m trainer_config \u001b[38;5;241m=\u001b[39m TrainerConfig(\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     10\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     data_aware_init_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m optimizer_config \u001b[38;5;241m=\u001b[39m OptimizerConfig()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'target_col' is not defined"
     ]
    }
   ],
   "source": [
    "data_config = DataConfig(\n",
    "    target=[\n",
    "        target_col\n",
    "    ],  # target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    batch_size=1024,\n",
    "    max_epochs=25,\n",
    "    auto_lr_find=True,\n",
    "    early_stopping=\"valid_loss\",  # Monitor valid_loss for early stopping\n",
    "    early_stopping_mode=\"min\",  # Set the mode as min because for val_loss, lower is better\n",
    "    early_stopping_patience=5,  # No. of epochs of degradation training will wait before terminating\n",
    "    checkpoints=\"valid_loss\",  # Save best checkpoint monitoring val_loss\n",
    "    load_best=True,  # After training, load the best checkpoint\n",
    "    progress_bar=\"none\",  # Turning off Progress bar\n",
    "    trainer_kwargs=dict(enable_model_summary=False),  # Turning off model summary\n",
    "    accelerator=\"cpu\",\n",
    "    fast_dev_run=True,\n",
    "    data_aware_init_batch_size=1024,\n",
    ")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\", dropout=0.1, initialization=\"kaiming\"  # No additional layer in head, just a mapping layer to output_dim\n",
    ").__dict__  # Convert to dict to pass to the model config (OmegaConf doesn't accept objects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Sweep API\n",
    "\n",
    "<!-- Args:\n",
    "    task (str): The type of prediction task. Either 'classification' or 'regression'\n",
    "\n",
    "    train (pd.DataFrame): The training data\n",
    "\n",
    "    test (pd.DataFrame): The test data on which performance is evaluated\n",
    "\n",
    "    data_config (Union[DataConfig, str]): DataConfig object or path to the yaml file.\n",
    "\n",
    "    optimizer_config (Union[OptimizerConfig, str]): OptimizerConfig object or path to the yaml file.\n",
    "\n",
    "    trainer_config (Union[TrainerConfig, str]): TrainerConfig object or path to the yaml file.\n",
    "\n",
    "    models (Union[str, List[Union[ModelConfig, str]]], optional): The list of models to compare. This can be one of\n",
    "            the presets defined in ``pytorch_tabular.MODEL_SWEEP_PRESETS`` or a list of ``ModelConfig`` objects.\n",
    "            Defaults to \"fast\".\n",
    "\n",
    "    metrics (Optional[List[str]]): the list of metrics you need to track during training. The metrics\n",
    "            should be one of the functional metrics implemented in ``torchmetrics``. By default, it is\n",
    "            accuracy if classification and mean_squared_error for regression\n",
    "\n",
    "    metrics_prob_input (Optional[bool]): Is a mandatory parameter for classification metrics defined in\n",
    "            the config. This defines whether the input to the metric function is the probability or the class.\n",
    "            Length should be same as the number of metrics. Defaults to None.\n",
    "\n",
    "    metrics_params (Optional[List]): The parameters to be passed to the metrics function. `task` is forced to\n",
    "            be `multiclass` because the multiclass version can handle binary as well and for simplicity we are\n",
    "            only using `multiclass`.\n",
    "\n",
    "    validation (Optional[DataFrame], optional):\n",
    "            If provided, will use this dataframe as the validation while training.\n",
    "            Used in Early Stopping and Logging. If left empty, will use 20% of Train data as validation.\n",
    "            Defaults to None.\n",
    "\n",
    "    experiment_config (Optional[Union[ExperimentConfig, str]], optional): ExperimentConfig object or path to the yaml file.\n",
    "\n",
    "    common_model_args (Optional[dict], optional): The model argument which are common to all models. The list of params can\n",
    "        be found in ``ModelConfig``. If not provided, will use defaults. Defaults to {}.\n",
    "\n",
    "    rank_metric (Optional[Tuple[str, str]], optional): The metric to use for ranking the models. The first element of the tuple\n",
    "        is the metric name and the second element is the direction. Defaults to ('loss', \"lower_is_better\").\n",
    "\n",
    "    return_best_model (bool, optional): If True, will return the best model. Defaults to True.\n",
    "\n",
    "    seed (int, optional): The seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    ignore_oom (bool, optional): If True, will ignore the Out of Memory error and continue with the next model. -->\n",
    "\n",
    "The model sweep enables you to quickly sweep thorugh different models and configurations. It takes in a list of model configs or one of the presets defined in ``model_comparator.MODEL_PRESETS`` and trains them on the data. It then ranks the models based on the metric provided and returns the best model.\n",
    "\n",
    "These are the major args:\n",
    "- ``task``: The type of prediction task. Either 'classification' or 'regression'\n",
    "- ``train``: The training data\n",
    "- ``test``: The test data on which performance is evaluated\n",
    "- all the config objects can be passed as either the object or the path to the yaml file.\n",
    "- ``models``: The list of models to compare. This can be one of the presets defined in ``pytorch_tabular.MODEL_SWEEP_PRESETS`` or a list of ``ModelConfig`` objects.\n",
    "- ``metrics``: the list of metrics you need to track during training. The metrics should be one of the functional metrics implemented in ``torchmetrics``. By default, it is accuracy if classification and mean_squared_error for regression\n",
    "- ``metrics_prob_input``: Is a mandatory parameter for classification metrics defined in the config. This defines whether the input to the metric function is the probability or the class. Length should be same as the number of metrics. Defaults to None.\n",
    "- ``metrics_params``: The parameters to be passed to the metrics function. \n",
    "- ``rank_metric``: The metric to use for ranking the models. The first element of the tuple is the metric name and the second element is the direction. Defaults to ('loss', \"lower_is_better\").\n",
    "- ``return_best_model``: If True, will return the best model. Defaults to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lite', 'full', 'high_memory'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "MODEL_SWEEP_PRESETS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sweep_df, best_model \u001b[38;5;241m=\u001b[39m model_sweep(\n\u001b[1;32m      2\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# One of \"classification\", \"regression\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m      4\u001b[0m     test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m      5\u001b[0m     data_config\u001b[38;5;241m=\u001b[39mdata_config,\n\u001b[1;32m      6\u001b[0m     optimizer_config\u001b[38;5;241m=\u001b[39moptimizer_config,\n\u001b[1;32m      7\u001b[0m     trainer_config\u001b[38;5;241m=\u001b[39mtrainer_config,\n\u001b[1;32m      8\u001b[0m     model_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     common_model_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(head\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearHead\u001b[39m\u001b[38;5;124m\"\u001b[39m, head_config\u001b[38;5;241m=\u001b[39mhead_config),\n\u001b[1;32m     10\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     metrics_params\u001b[38;5;241m=\u001b[39m[{}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m}],\n\u001b[1;32m     12\u001b[0m     metrics_prob_input\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m],\n\u001b[1;32m     13\u001b[0m     rank_metric\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigher_is_better\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m     progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "sweep_df, best_model = model_sweep(\n",
    "    task=\"classification\",  # One of \"classification\", \"regression\"\n",
    "    train=train,\n",
    "    test=test,\n",
    "    data_config=data_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    model_list=\"full\",\n",
    "    common_model_args=dict(head=\"LinearHead\", head_config=head_config),\n",
    "    metrics=['accuracy', \"f1_score\"],\n",
    "    metrics_params=[{}, {\"average\": \"weighted\"}],\n",
    "    metrics_prob_input=[False, True],\n",
    "    rank_metric=(\"accuracy\", \"higher_is_better\"),\n",
    "    progress_bar=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sweep_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sweep_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_taken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39mhighlight_max(\n\u001b[1;32m      2\u001b[0m     subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_f1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\u001b[38;5;241m.\u001b[39mhighlight_min(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlightgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sweep_df' is not defined"
     ]
    }
   ],
   "source": [
    "sweep_df.drop(columns=[\"params\", \"time_taken\", \"epochs\"]).style.highlight_max(\n",
    "    subset=[\"test_accuracy\", \"test_f1_score\"], color=\"lightgreen\"\n",
    ").highlight_min(subset=[\"test_loss\"], color=\"lightgreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the `lite` preset which is a set of four models which have comparable # of params and trains relatively faster with less memory requirements.\n",
    "\n",
    "We can see that GANDALF performs the best in terms of accuracy, loss and f1 score. We can also see that the training time is comparable to regular MLP. A natural next step would be to tune the model a but more and find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = CategoryEmbeddingModelConfig(\n",
    "    task=\"classification\",\n",
    "    layers=\"256-128-64\",\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    ")\n",
    "\n",
    "danet = DANetConfig(\n",
    "    task=\"classification\",\n",
    "    n_layers=8,\n",
    "    abstlay_dim_1=8,\n",
    "    k=5,\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    ")\n",
    "\n",
    "gandalf = GANDALFConfig(\n",
    "    task=\"classification\",\n",
    "    gflu_stages=6,\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    ")\n",
    "\n",
    "tabnet = TabNetModelConfig(\n",
    "    task=\"classification\",\n",
    "    n_d=32,\n",
    "    n_a=32,\n",
    "    n_steps=3,\n",
    "    gamma=1.5,\n",
    "    n_independent=1,\n",
    "    n_shared=2,\n",
    "    head=\"LinearHead\",\n",
    "    head_config=head_config,\n",
    ")\n",
    "model_list = [mlp, danet, gandalf, tabnet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabular import available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoIntConfig',\n",
       " 'CategoryEmbeddingModelConfig',\n",
       " 'DANetConfig',\n",
       " 'FTTransformerConfig',\n",
       " 'GANDALFConfig',\n",
       " 'GatedAdditiveTreeEnsembleConfig',\n",
       " 'TabNetModelConfig',\n",
       " 'TabTransformerConfig']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in available_models() if m not in [\"MDNConfig\", \"NodeConfig\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8d5d2789703c7b1c2f7bfaada1cbd3aa0ac53e2e4e1cae5da195f5520da229"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
